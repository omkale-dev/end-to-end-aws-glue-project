{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "import json\n",
    "import importlib\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import helper_functions\n",
    "importlib.reload(helper_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_bucket = \"pf-gold\"\n",
    "database_name = \"pf_gold_db\"\n",
    "crawler_name = \"pf-gold-crawler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_init():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    glueContext = GlueContext(sc, region='us-east-1')\n",
    "    # spark = glueContext.spark_session\n",
    "    job = Job(glueContext)\n",
    "    return spark, glueContext, job\n",
    "\n",
    "\n",
    "def custom_quarter(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 1\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 2\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 3\n",
    "    elif month in [12, 1, 2]:\n",
    "        return 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "quarter_udf = udf(custom_quarter, IntegerType())\n",
    "\n",
    "\n",
    "def get_date_dim(start_date, end_date):\n",
    "    # Convert date strings to datetime objects\n",
    "    start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date_obj = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    # Validate input dates\n",
    "    if start_date_obj >= end_date_obj:\n",
    "        raise ValueError(\"End date must be after start date.\")\n",
    "\n",
    "    # Calculate the difference in days\n",
    "    delta = end_date_obj - start_date_obj\n",
    "    total_days = delta.days\n",
    "    date_range = spark.range(0, total_days + 1).selectExpr(\n",
    "        f\"date_add(to_date('{start_date}'), CAST(id AS INT)) as date\")\n",
    "    # Extract date attributes\n",
    "    date_dimension = date_range \\\n",
    "        .withColumn(\"year\", year(col(\"date\"))) \\\n",
    "        .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "        .withColumn(\"day\", dayofmonth(col(\"date\"))) \\\n",
    "        .withColumn(\"day_of_week\", dayofweek(col(\"date\"))) \\\n",
    "        .withColumn(\"week_of_year\", weekofyear(col(\"date\")))\\\n",
    "        .withColumn(\"quarter\", quarter_udf(col(\"month\")))\n",
    "    return date_dimension\n",
    "\n",
    "\n",
    "def load(df):\n",
    "    path = f\"s3a://{delta_bucket}/dim_date/\"\n",
    "    additional_options = {\n",
    "        \"path\": path\n",
    "    }\n",
    "    # to hard overwrite, simply adding overwrite will fail if some table metadata is not found\n",
    "    helper_functions.empty_s3_path(delta_bucket, \"dim_date\")\n",
    "    # df.write \\\n",
    "    #     .format(\"delta\") \\\n",
    "    #     .options(**additional_options) \\\n",
    "    #     .mode(\"overwrite\") \\\n",
    "    #     .saveAsTable(f\"`{database_name}`.dim_date\")\n",
    "    # Note:As of 05/24 theres a known issue with writing directly to table using saveAsTable https://github.com/datahub-project/datahub/pull/10299\n",
    "    # So we will have to use crawler.\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(path)\n",
    "    df.show()\n",
    "    # Start the crawler\n",
    "    client = boto3.client('glue')\n",
    "    print(f\"Starting the crawler: {crawler_name}\")\n",
    "    response = client.start_crawler(Name=crawler_name)\n",
    "    print(response)\n",
    "    print(f\"dim_date was loaded successfully\")\n",
    "\n",
    "\n",
    "def send_event_to_eventbridge(run_id, status, process_update):\n",
    "    # Create an EventBridge client\n",
    "    client = boto3.client('events')\n",
    "    if status == 'failed':\n",
    "        message = f'An error was occured while processing dim date. Error:{process_update}'\n",
    "\n",
    "    else:\n",
    "        message = f'Processing dim date was successfull:{process_update}'\n",
    "    detail = {\n",
    "        'run_id': run_id,\n",
    "        'producer': 'silver-2-gold',\n",
    "        'status': status,\n",
    "        'message': message,\n",
    "\n",
    "    }\n",
    "    # Define the event\n",
    "    event = {\n",
    "        \"Source\": \"pf-resources\",\n",
    "        \"DetailType\": \"pf-resource-event\",\n",
    "        \"Detail\": json.dumps(detail),\n",
    "    }\n",
    "    # Send the event\n",
    "    response = client.put_events(Entries=[event])\n",
    "\n",
    "\n",
    "def process_dim_date(spark, start_date, end_date):\n",
    "    df = get_date_dim(start_date, end_date)\n",
    "    load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "# if __name__ == \"__main__\":\n",
    "error = False\n",
    "error_msg = \" \"\n",
    "spark, glueContext, job = glue_init()\n",
    "\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2025-12-31\"\n",
    "\n",
    "try:\n",
    "    process_dim_date(spark, start_date, end_date)\n",
    "except Exception as E:\n",
    "    exception_details = E.args[0] if E.args else \"Unknown error\"\n",
    "    print(E)\n",
    "    error_msg += f\"There was issue while processing dim date \\nERROR: {exception_details}\\n\"\n",
    "    print(error_msg)\n",
    "    error = True\n",
    "if error:\n",
    "    send_event_to_eventbridge(0, 'failed', error_msg)\n",
    "else:\n",
    "    send_event_to_eventbridge(0, 'success', 'ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
