{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.job import Job\n",
    "import boto3\n",
    "import json\n",
    "import importlib\n",
    "import helper_functions\n",
    "importlib.reload(helper_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "LOCAL = True\n",
    "global run_id\n",
    "run_id = 1\n",
    "bronze_bucket = \"pf-bronze\"\n",
    "landing_bucket = \"pf-landing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def glue_init():\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    glueContext = GlueContext(sc)\n",
    "    spark = glueContext.spark_session\n",
    "    job = Job(glueContext)\n",
    "    if not LOCAL:\n",
    "        global run_id\n",
    "        args = getResolvedOptions(sys.argv, [\"run_id\"])\n",
    "        run_id = args[\"run_id\"]\n",
    "    return spark, glueContext, job\n",
    "\n",
    "\n",
    "def send_event_to_eventbridge(run_id, status, process_update):\n",
    "    # Create an EventBridge client\n",
    "    client = boto3.client('events')\n",
    "    if status == 'failed':\n",
    "        message = f'An error was occured while transfering files from landing to bronze. Error:{process_update}'\n",
    "\n",
    "    else:\n",
    "        message = f'The data transfer from landing to bronze was successfull:{process_update}'\n",
    "    detail = {\n",
    "        'run_id': str(run_id),\n",
    "        'producer': 'landing-2-bronze',\n",
    "        'status': status,\n",
    "        'message': message,\n",
    "\n",
    "    }\n",
    "    # Define the event\n",
    "    event = {\n",
    "        \"Source\": \"pf-resources\",\n",
    "        \"DetailType\": \"pf-resource-event\",\n",
    "        \"Detail\": json.dumps(detail),\n",
    "    }\n",
    "    # Send the event\n",
    "    response = client.put_events(Entries=[event])\n",
    "\n",
    "\n",
    "def get_files():\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_objects_v2(Bucket=landing_bucket)\n",
    "    json_files = []\n",
    "    for obj in response.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        if key.endswith('.json'):\n",
    "            json_files.append(key)\n",
    "    return json_files\n",
    "\n",
    "\n",
    "def extract_table_name(file_path):\n",
    "    parts = file_path.split('/')\n",
    "    file_name_with_extension = parts[-1]\n",
    "    name_parts = file_name_with_extension.split('.')\n",
    "    table_name = name_parts[0]\n",
    "    return table_name\n",
    "\n",
    "\n",
    "def convert_to_parquet(spark, files):\n",
    "    for file in files:\n",
    "        table_name = extract_table_name(file)\n",
    "        df = spark.read.json(f\"s3://{landing_bucket}/{file}\")\n",
    "        bronze_file_path = f\"s3a://{bronze_bucket}/{run_id}/{table_name}\"\n",
    "        df.write.mode('overwrite').parquet(bronze_file_path)\n",
    "        print(\"Files were converted and loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# if __name__==\"__main__\":\n",
    "helper_functions.test()\n",
    "error = False\n",
    "spark, glueContext, job = glue_init()\n",
    "try:\n",
    "    convert_to_parquet(spark, get_files())\n",
    "except Exception as e:\n",
    "    send_event_to_eventbridge(run_id, 'failed', e)\n",
    "    error = True\n",
    "if not error:\n",
    "    send_event_to_eventbridge(run_id, 'success', 'ok')\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
