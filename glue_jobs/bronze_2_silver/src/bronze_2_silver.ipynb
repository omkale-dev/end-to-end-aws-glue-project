{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql.functions import col, expr, to_date, round, substring, length, trim, current_timestamp, date_format\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "import json\n",
    "import importlib\n",
    "import boto3\n",
    "from pyspark.sql.functions import col, coalesce, lit\n",
    "import helper_functions\n",
    "importlib.reload(helper_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "LOCAL = True\n",
    "global run_id, mapping_path, load_type\n",
    "run_id = 3\n",
    "load_type = 'full_load'\n",
    "job_name = \"bronze_2_silver\"\n",
    "bronze_bucket = \"pf-bronze\"\n",
    "mapping_path = f\"./jupyter_workspace/{job_name}/resources/mapping.csv\"\n",
    "delta_bucket = \"pf-silver\"\n",
    "database_name = \"pf_silver_db\"\n",
    "dq_failed_bucket = \"pf-dq-failed\"\n",
    "crawler_name = \"pf-silver-crawler\"\n",
    "\n",
    "select_cast = {\n",
    "    \"date\": [DateType(), 'DATE'],\n",
    "    \"string\": [StringType(), 'STRING'],\n",
    "    \"integer\": [LongType(), 'LONG'],  # for now\n",
    "    \"float\": [FloatType(), 'FLOAT']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def glue_init():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    glueContext = GlueContext(sc, region='us-east-1')\n",
    "    # spark = glueContext.spark_session\n",
    "    job = Job(glueContext)\n",
    "    if not LOCAL:\n",
    "        global run_id, mapping_path, load_type\n",
    "        args = getResolvedOptions(sys.argv, [\"run_id\", \"load_type\"])\n",
    "        run_id = args[\"run_id\"]\n",
    "        load_type = args[\"load_type\"]\n",
    "        print(run_id, load_type)\n",
    "        mapping_path = \"./mapping.csv\"\n",
    "\n",
    "    return spark, glueContext, job\n",
    "\n",
    "\n",
    "def extract_data(glueContext, source_table_path):\n",
    "    source_df = helper_functions.get_source_df(glueContext, source_table_path)\n",
    "    return source_df\n",
    "\n",
    "\n",
    "def clean_data(glueContext, source_df, mapping, table_name):\n",
    "    select_list = []\n",
    "    for row in mapping:\n",
    "        if row['source_table'] == table_name and row['is_generated'] == 'N':\n",
    "            # 1. dropping duplicates\n",
    "            source_df = source_df.dropDuplicates()\n",
    "            # 2. filling null values with default values\n",
    "            source_df = source_df.fillna(\n",
    "                {row['source_column']: row['na_default']})\n",
    "            # 3. ensuring the correct datatypes\n",
    "            if row['cast_to'] == 'date':\n",
    "                select_list.append(\n",
    "                    to_date(col(row['source_column']), \"yyyy-MM-dd\").alias(row['source_column']))\n",
    "            # 4. ensuring data standards\n",
    "            elif row['cast_to'] == 'float':\n",
    "                select_list.append(round(col(row['source_column']).cast(\n",
    "                    select_cast[row['cast_to']][0]), 2).alias(row['source_column']))\n",
    "            else:\n",
    "                select_list.append(col(row['source_column']).cast(\n",
    "                    select_cast[row['cast_to']][0]).alias(row['source_column']))\n",
    "\n",
    "    # 5. selecting only required data\n",
    "    cleaned_df = source_df.select(select_list)\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "def transform_data(glueContext, cleaned_df, mapping, table_name):\n",
    "    select_list = []\n",
    "    for row in mapping:\n",
    "        if row['source_table'] == table_name:\n",
    "            # 1. predefined transformations\n",
    "            if row['transformation'] == \"PHONE\":\n",
    "                select_list.append(\n",
    "                    substring(row['target_column'], -10, 10).alias(row['target_column']))\n",
    "            # 2. expression based transformations\n",
    "            elif row['transformation'] == 'EXPR':\n",
    "                # eval expr, then cast, then alias\n",
    "                select_list.append(expr(\n",
    "                    f\"CAST({row['expr']} AS {select_cast[row['cast_to']][1]}) AS {row['target_column']}\"))\n",
    "            # 3. the custom transformation would have gone here...\n",
    "            elif row['transformation'] == \"CUSTOM\":\n",
    "                pass\n",
    "            else:\n",
    "                # 4. mapping source with target column\n",
    "                select_list.append(\n",
    "                    col(row['source_column']).alias(row['target_column']))\n",
    "    # 5. add last update timestamp\n",
    "    select_list.append(date_format(current_timestamp(),\n",
    "                       \"yyyy-MM-dd HH:mm:ss\").alias(\"last_updated\"))\n",
    "    # 6. add etl run_id\n",
    "    select_list.append(lit(run_id).alias('etl_run_id'))\n",
    "    transformed_df = cleaned_df.select(select_list)\n",
    "    return transformed_df\n",
    "\n",
    "\n",
    "def check_dq(glueContext, transformed_df, mapping, table_name):\n",
    "    from awsgluedq.transforms import EvaluateDataQuality\n",
    "    rule_list = []\n",
    "    for row in mapping:\n",
    "        if row['source_table'] == table_name:\n",
    "            if row['dq_rule'] != \"NA\":\n",
    "                rule_list.append(row['dq_rule'])\n",
    "    if len(rule_list) > 0:  # if there are no dq rules then skip the process\n",
    "        EvaluateDataQuality_ruleset = helper_functions.generate_ruleset_string(\n",
    "            rule_list)  # the format: EvaluateDataQuality_ruleset=\"\"\"Rules = [ColumnValues \"gender\" in [\"f\",\"m\"],ColumnValues \"cattle_capacity\" >= 2]\"\"\"\n",
    "        print(EvaluateDataQuality_ruleset)\n",
    "\n",
    "        dyf = DynamicFrame.fromDF(transformed_df, glueContext, \"dyf\")\n",
    "\n",
    "        # applying dq check wrt ruleset\n",
    "        EvaluateDataQualityMultiframe = EvaluateDataQuality().process_rows(\n",
    "            frame=dyf,\n",
    "            ruleset=EvaluateDataQuality_ruleset,\n",
    "            publishing_options={\n",
    "                \"dataQualityEvaluationContext\": \"EvaluateDataQualityMultiframe\",\n",
    "                \"enableDataQualityCloudWatchMetrics\": False,\n",
    "                \"enableDataQualityResultsPublishing\": False,\n",
    "            },\n",
    "            additional_options={\"performanceTuning.caching\": \"CACHE_NOTHING\"},\n",
    "        )\n",
    "\n",
    "        # result of dq\n",
    "        ruleOutcomes = SelectFromCollection.apply(\n",
    "            dfc=EvaluateDataQualityMultiframe,\n",
    "            key=\"ruleOutcomes\",\n",
    "            transformation_ctx=\"ruleOutcomes\",\n",
    "        )\n",
    "        ruleOutcomes.toDF().show(truncate=False)\n",
    "\n",
    "        # to get df out of dyf\n",
    "        rowLevelOutcomes = SelectFromCollection.apply(\n",
    "            dfc=EvaluateDataQualityMultiframe,\n",
    "            key=\"rowLevelOutcomes\",\n",
    "            transformation_ctx=\"rowLevelOutcomes\",\n",
    "        )\n",
    "\n",
    "        # the passed and failed records\n",
    "        # Convert Glue DynamicFrame to SparkSQL DataFrame\n",
    "        rowLevelOutcomes_df = rowLevelOutcomes.toDF()\n",
    "        rowLevelOutcomes_df_passed = rowLevelOutcomes_df.filter(\n",
    "            rowLevelOutcomes_df.DataQualityEvaluationResult == \"Passed\")  # Filter only the Passed records.\n",
    "        rowLevelOutcomes_df_failed = rowLevelOutcomes_df.filter(\n",
    "            rowLevelOutcomes_df.DataQualityEvaluationResult == \"Failed\")  # Review the Failed records\n",
    "\n",
    "        # dropping the dq related columns\n",
    "        columns_to_drop = ['DataQualityRulesPass', 'DataQualityRulesFail',\n",
    "                           'DataQualityRulesSkip', 'DataQualityEvaluationResult']  # we dont need these dq results\n",
    "        # Selecting all columns except the specified ones\n",
    "        selected_columns = [\n",
    "            col for col in rowLevelOutcomes_df_passed.columns if col not in columns_to_drop]\n",
    "        # Creating the DataFrame with selected columns\n",
    "        rowLevelOutcomes_df_passed = rowLevelOutcomes_df_passed.select(\n",
    "            selected_columns)\n",
    "\n",
    "        # rowLevelOutcomes_df_passed.show(5)\n",
    "        # rowLevelOutcomes_df_failed.show(5)\n",
    "        helper_functions.save_failed_dq_records(\n",
    "            rowLevelOutcomes_df_failed, dq_failed_bucket, table_name, run_id)  # saving the data to s3\n",
    "        return rowLevelOutcomes_df_passed\n",
    "    else:\n",
    "        return transformed_df\n",
    "\n",
    "\n",
    "# used to create update and insert set\n",
    "def delta_set(table_name, mapping):\n",
    "    whenNotMatchedInsert_set = {}\n",
    "    whenMatchedUpdate_set = {}\n",
    "    for row in mapping:\n",
    "        if row['source_table'] == table_name:\n",
    "            prev_col_name = f\"prev_df.{row['target_column']}\"\n",
    "            append_col_name = f\"append_df.{row['target_column']}\"\n",
    "            if row['target_column'] != 'dwid':  # we dont want to update the dwid\n",
    "                whenMatchedUpdate_set[prev_col_name] = col(append_col_name)\n",
    "            whenNotMatchedInsert_set[prev_col_name] = col(append_col_name)\n",
    "    # adding the etl_run_id and last_updated\n",
    "    whenMatchedUpdate_set[\"prev_df.etl_run_id\"] = col(\"append_df.etl_run_id\")\n",
    "    whenMatchedUpdate_set[\"prev_df.last_updated\"] = col(\n",
    "        \"append_df.last_updated\")\n",
    "    whenNotMatchedInsert_set[\"prev_df.last_updated\"] = col(\n",
    "        \"append_df.last_updated\")\n",
    "    whenNotMatchedInsert_set[\"prev_df.etl_run_id\"] = col(\n",
    "        \"append_df.etl_run_id\")\n",
    "    return whenNotMatchedInsert_set, whenMatchedUpdate_set\n",
    "\n",
    "\n",
    "def load(table_name, transformed_df, whenNotMatchedInsert_set, whenMatchedUpdate_set):\n",
    "    path = f\"s3a://{delta_bucket}/{table_name}/\"\n",
    "    if load_type == 'full_load':\n",
    "        additional_options = {\n",
    "            \"path\": path\n",
    "        }\n",
    "        # to hard overwrite, simply adding overwrite will fail if some table metadata is not found\n",
    "        helper_functions.empty_s3_path(delta_bucket, table_name)\n",
    "        # transformed_df.write \\\n",
    "        #     .format(\"delta\") \\\n",
    "        #     .options(**additional_options) \\\n",
    "        #     .mode(\"overwrite\") \\\n",
    "        #     .saveAsTable(f\"`{database_name}`.{table_name}\")\n",
    "        # Note:As of 05/24 theres a known issue with writing directly to table using saveAsTable https://github.com/datahub-project/datahub/pull/10299\n",
    "        # So we will have to use crawler.\n",
    "        transformed_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(path)\n",
    "\n",
    "        # Start the crawler\n",
    "        run_crawler = True\n",
    "\n",
    "        print(f\"full_load for {table_name} was successfull\")\n",
    "    else:\n",
    "        delta_df = DeltaTable.forPath(\n",
    "            spark,  f\"s3://{delta_bucket}\" + f\"/{table_name}/\")\n",
    "        delta_df.alias(\"prev_df\").merge(\n",
    "            source=transformed_df.alias(\"append_df\"), \\\n",
    "            # matching on dwid\n",
    "            condition=expr(\"prev_df.dwid = append_df.dwid\"))\\\n",
    "            .whenMatchedUpdate(\n",
    "            set=whenMatchedUpdate_set\n",
    "        )\\\n",
    "            .whenNotMatchedInsert(\n",
    "            values=whenNotMatchedInsert_set\n",
    "        ).execute()\n",
    "        print(f\"SCD1 load for {table_name} was successfull\")\n",
    "\n",
    "\n",
    "def send_event_to_eventbridge(run_id, status, process_update):\n",
    "    # Create an EventBridge client\n",
    "    client = boto3.client('events')\n",
    "    if status == 'failed':\n",
    "        message = f'An error was occured while transfering files from bronze to silver. Error:{process_update}'\n",
    "\n",
    "    else:\n",
    "        message = f'The data transfer from bronze to silver was successfull:{process_update}'\n",
    "    detail = {\n",
    "        'run_id': str(run_id),\n",
    "        'producer': 'bronze-2-silver',\n",
    "        'status': status,\n",
    "        'message': message,\n",
    "\n",
    "    }\n",
    "    # Define the event\n",
    "    event = {\n",
    "        \"Source\": \"pf-resources\",\n",
    "        \"DetailType\": \"pf-resource-event\",\n",
    "        \"Detail\": json.dumps(detail),\n",
    "    }\n",
    "    # Send the event\n",
    "    response = client.put_events(Entries=[event])\n",
    "\n",
    "\n",
    "def process_tables(glueContext, mapping, table_name):\n",
    "    # for each table path in mapping\n",
    "    source_table_path = f\"s3://{bronze_bucket}/{run_id}/{table_name}\"\n",
    "\n",
    "    source_df = extract_data(glueContext, source_table_path)\n",
    "    # source_df = source_df.union(helper_functions.get_null_df(spark)) # for testing\n",
    "    # print(f\"{table_name}:source\")\n",
    "    # source_df.show()\n",
    "    # source_df.printSchema()\n",
    "\n",
    "    cleaned_df = clean_data(glueContext, source_df, mapping, table_name)\n",
    "    # cleaned_df.printSchema()\n",
    "    # print(f\"{table_name}:cleaned\")\n",
    "    # cleaned_df.show()\n",
    "\n",
    "    transformed_df = transform_data(\n",
    "        glueContext, cleaned_df, mapping, table_name)\n",
    "    # print(f\"{table_name}:transformed\")\n",
    "    # transformed_df.show()\n",
    "\n",
    "    whenNotMatchedInsert_set, whenMatchedUpdate_set = delta_set(\n",
    "        table_name, mapping)\n",
    "\n",
    "    if not LOCAL:  # as the dq will not work locally\n",
    "        dq_passed_df = check_dq(\n",
    "            glueContext, transformed_df, mapping, table_name)\n",
    "    else:\n",
    "        dq_passed_df = transformed_df\n",
    "\n",
    "    load(table_name, dq_passed_df, whenNotMatchedInsert_set, whenMatchedUpdate_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "# if __name__ == \"__main__\":\n",
    "error = False\n",
    "error_msg = \" \"\n",
    "spark, glueContext, job = glue_init()\n",
    "\n",
    "mapping = helper_functions.get_csv_mapping(mapping_path)\n",
    "tables = set()\n",
    "for row in mapping:\n",
    "    tables.add(row['source_table'])\n",
    "for index, table in enumerate(tables):\n",
    "    # if table == 'aggregated_procurement': # for test used: aggregated_procurement\n",
    "    # if table == 'collection_center_procurement':\n",
    "    try:\n",
    "        process_tables(glueContext, mapping, table)\n",
    "        if index == len(tables)-1:\n",
    "            client = boto3.client('glue')\n",
    "            print(f\"Starting the crawler: {crawler_name}\")\n",
    "            response = client.start_crawler(Name=crawler_name)\n",
    "            print(response)\n",
    "    except Exception as E:\n",
    "        print(E)\n",
    "        exception_details = E.args[0] if E.args else \"Unknown error\"\n",
    "        error_msg += f\"There was issue while processing table:{table}\\nERROR: {exception_details}\\n\"\n",
    "        print(error_msg)\n",
    "        error = True\n",
    "if error:\n",
    "    send_event_to_eventbridge(run_id, 'partial', error_msg)\n",
    "else:\n",
    "    send_event_to_eventbridge(run_id, 'success', 'ok')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
